{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af116b7-bdd4-454a-9f13-06c6f1c79540",
   "metadata": {},
   "source": [
    "# DAwHPC Practical 08 - Features, Trees, and Forests\n",
    "\n",
    "In this practical you will investigate building Decision Trees, Pruning, and building Random Forests using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a0065-201f-45db-9d3e-e9aafc7d110c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f65c1b-56b4-4d9c-96ca-aaeca8c54fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e648d-eaf0-4c88-b080-b6e9ad24cfe4",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "###  Part 1 - Introduction and Data Preparation\n",
    "\n",
    "We’re going to use the Carseats dataset, which is provided as a CSV. This dataset contains 200 observations detailing the sales of child car seats across many shops.\n",
    "\n",
    "Load and examine the variables in the dataset. You should see that it contains both continuous and categorical variables.\n",
    "\n",
    "### Note: One-hot (dummy) Encoding\n",
    "\n",
    "The sklearn tree classifiers don't currently deal with categorical features, so we need to use [\"One-Hot Encoding\"](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) in order to transform a categorical feature of `n` levels into `n` new features. For each row, the new features will all be `0` except for the one feature representing the original value.\n",
    "\n",
    "For example:\n",
    "\n",
    "| ID | Rating |\n",
    "| --- | --- |\n",
    "| 1 | Low| \n",
    "| 2 | High|\n",
    "| 3 | Medium |\n",
    "\n",
    "would be one-hot encoded to:\n",
    "\n",
    "| ID | Rating_Low | Rating_Medium | Rating_High |\n",
    "| -- | ---------- | ------------- | ----------- |\n",
    "| 1  | 1|0|0|\n",
    "| 2 |0|0|1|\n",
    "| 3|0|1|0|\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Load and examine the variables in the dataset. You should see that it contains both continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef06b33-7e2b-4cb8-a6a3-e4c31e61b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats_df = pd.read_csv(\"DatasetsForPracticals/DecisionTreesAndForests/carseats.csv\")\n",
    "\n",
    "# One-hot-encode ShelveLoc\n",
    "carseats_df = pd.get_dummies(carseats_df, columns=[\"ShelveLoc\"])\n",
    "\n",
    "# Map \"Yes\" and \"No\" to True/False\n",
    "to_bool = {'Yes': True, 'No': False}\n",
    "carseats_df[\"Urban\"] = carseats_df[\"Urban\"].map(to_bool)\n",
    "carseats_df[\"US\"] = carseats_df[\"US\"].map(to_bool)\n",
    "\n",
    "display(carseats_df.head())\n",
    "display(carseats_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08908ba2-f739-4f6b-b428-c777fd850ab4",
   "metadata": {},
   "source": [
    "The variables are defined as:\n",
    "- `Sales`: Unit sales (in thousands) at each location\n",
    "- `CompPrice`: Price charged by competitor at each location\n",
    "- `Income`: Community income level (in thousands of dollars)\n",
    "- `Advertising`: Local advertising budget for company at each location (in thousands of dollars)\n",
    "- `Population`: Population size in region (in thousands)\n",
    "- `Price`: Price company charges for car seats at each site\n",
    "- `ShelveLoc`: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\n",
    "- `Age`: Average age of the local population\n",
    "- `Education`: Education level at each location\n",
    "- `Urban`: A factor with levels No and Yes to indicate whether the store is in an urban or rural location\n",
    "- `US`: A factor with levels No and Yes to indicate whether the store is in the US or not\n",
    "\n",
    "We want to be able to determine which features are the best predictors for high or low sales at a given location. Before we build a tree which will do this for us though, think about which features could be strong predictors of high or low sales at a particular location. Will any features have little correlation with the sales value?\n",
    "\n",
    "Write-down your initial guesses so you can compare later.\n",
    "\n",
    "Before we can train a classification tree using our data, we need to define what value of the Sales variable constitutes “high” or “low”. For this exercise, let’s define high sales as being above `8`. Create this variable and append it to the Carseats dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900f8ce-fdda-4850-b409-8437ab7f6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats_df[\"HighSales\"] = carseats_df[\"Sales\"] > 8\n",
    "carseats_df[\"HighSales\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4235a-700b-4c9a-ae9a-169cb2867e3e",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "In the lecture, we discussed that there are a few possible feature selection criteria which Decision Trees use to best split the remaining data points at each node. We worked-through an example using Entropy and Information Gain.\n",
    "\n",
    "In this lab, we’re going to be using another method, called the _Gini Index_. This captures the probability of a\n",
    "data point being incorrectly classified when it is randomly chosen. It is defined by\n",
    "\n",
    "$$\\Large G = \\sum_{k=1}^K p_k(1-p_k)$$\n",
    "\n",
    "where K is the set of possible classes, and p_k is the probability of the class being chosen.\n",
    "\n",
    "Similarly to the definition of Entropy for a binary event, the Gini index is symmetric. It has value 0 at 0% or\n",
    "100% probability, and 0.5 at 50%. See Figure 2 below.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/gini.png\"/>\n",
    "    <figcaption>Figure 2</figcaption>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "At each node the Gini index is calculated for each of the remaining available features, and the feature which\n",
    "results in the lowest value is chosen as the feature to split on. This is much simpler than the Information Gain\n",
    "approach, which selects the next feature based on the largest reduction to the initial entropy at the node.\n",
    "\n",
    "Information Gain tends to end-up splitting data to find the maximum reduction to impurity, which can result in smaller trees than Gini. Gini indexes don’t include any logarithm terms though, which are relatively expensive to compute. This can be a factor when training on large datasets. In general, both approaches produce trees with similar accuracies within a few percent [2].\n",
    "\n",
    "### Part 2 - Classification Tree\n",
    "\n",
    "We’re now ready to use the tree function to build and fit a classification tree to our data. As discussed, the Decision Tree algorithm will automatically pick the best features at each node in order to best split the data.\n",
    "\n",
    "We’ll therefore give it all the features except Sales. We’ll name this one `tree_simple`, which will be justified in the next section.\n",
    "\n",
    "**Note**: We have used Python's [dictionary unpacking](https://reference.codeproject.com/python3/dictionaries/python-dictionary-unpack) syntax below to help pass and re-use the required function parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2b3fa-ce71-4dac-b208-462301347316",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = carseats_df.drop(columns=[\"Sales\", \"HighSales\"])\n",
    "y = carseats_df[\"HighSales\"]\n",
    "\n",
    "tree_params = {\n",
    "    \"criterion\": \"gini\",\n",
    "    \"max_depth\": 6,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "tree_simple = sklearn.tree.DecisionTreeClassifier(**tree_params)\n",
    "tree_simple.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cbd1f-f827-476e-a5b8-744d72949bcf",
   "metadata": {},
   "source": [
    "We can now examine the resulting tree to see which features it has used, and its error rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f12ee-fd2f-4e51-9a81-57f047a50c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\")\n",
    "tree_importances = pd.Series(tree_simple.feature_importances_, index=X.columns)\n",
    "display(tree_importances.sort_values(ascending=False))\n",
    "\n",
    "print()\n",
    "print(\"Depth: \", tree_simple.get_depth())\n",
    "print(\"Leaf nodes: \", tree_simple.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c96b9-5ce1-42d2-9528-c9cca794f3a8",
   "metadata": {},
   "source": [
    "From the output, we can see:\n",
    "- The features the tree actually used to categorise the data, ranked by importance\n",
    "- The number of terminal or “leaf” nodes - 35 here\n",
    "\n",
    "Compare the features which the tree selected with your initial guesses from Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc652fb-f493-43d3-923f-8b757f2eee49",
   "metadata": {},
   "source": [
    "We can also plot the tree to visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3feea-3537-49bc-b310-4ae79eef23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sklearn.tree.plot_tree(tree_simple, max_depth=2, filled=True, proportion=True, fontsize=10, feature_names=X.columns)\n",
    "plt.title(\"Carseats Decision Tree (Simple)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b20ea-72bb-4d29-8b17-e201ef5c166b",
   "metadata": {},
   "source": [
    "From the plot, you should see that the tree has calculated `ShelveLoc_Good` as the most important feature to start with when predicing `Sales`, followed by `Price` in each child node. This matches the summary output above. You can change the `max_depth` setting in `plot_tree` to show more tree levels. Also note that the tree is quite high and has many leaves. This could indicate overfittig, which we will address in Part 3.\n",
    "\n",
    "Let’s use the predict function to calculate predictions for the whole dataset, and create a confusion matrix to enable calculation of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29f96a-64d5-4d44-b33a-eec09234fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    tree_simple.predict(X),\n",
    "    y,\n",
    "    margins=True,\n",
    "    rownames=[\"Predicted\"],\n",
    "    colnames=[\"Actual\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53132e8-c61a-4c02-a4f9-0b8de075a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {tree_simple.score(X, y)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9389df-bf98-4562-9efb-40c1c0cc9004",
   "metadata": {},
   "source": [
    "The approach above isn’t really valid though, since we want to predict our tree’s performance on unseen data, and use that as a fair measure.\n",
    "\n",
    "To solve this, we need to split our data into training and testing datasets, then only train on the training subset, and measure the resulting prediction performance against the unseen testing data.\n",
    "\n",
    "**Exercise**\n",
    "1. Construct `carseats_train` and `carseats_test` dataframes randomly from the full dataset, choosing an 80/20 split\n",
    "1. Train a new decision tree, `tree_split`, using only the training dataset\n",
    "1. Evaluate the new tree, this time using the carseats_testing dataset. Does the \"new\" accuracy value surprise you? Consider how the calculated value has been affected by splitting our data into training and testing.\n",
    "\n",
    "### Part 3 - Pruning\n",
    "\n",
    "Let’s see if we can boost our tree’s accuracy. As discussed in the lecture, it’s possible that our tree may contain too many branches, and therefore be overfitted to the training data. This occurs when the tree algorithm continues to partition the data until the leaves contain a \n",
    "very small number of values for only one class. I.e. the tree has learned how to classify the training data and has become overfitted. We can try to prune our tree to see if our accuracy improves on the unseen testing data.\n",
    "\n",
    "We're going to use [\"Minimal Cost-Complexity Pruning\"](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning), and determine how pruning has an effect on our classifier.\n",
    "\n",
    "First, run a cost-complexity pruning to return the effective alphas used during pruning. We can also visualise how the tree impurity is affected by the pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fc935-b599-49b9-beee-4edb2df9e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_split = sklearn.tree.DecisionTreeClassifier(**tree_params)\n",
    "path = tree_split.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "plt.plot(ccp_alphas, impurities)\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total impurity of leaves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f7065-ef1d-4a4a-9cde-5a9ecdc7c04e",
   "metadata": {},
   "source": [
    "Now we can feed the effective alpha values into new classifiers, and determine the value which gives the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628b687-3f8e-4050-91d9-ef3a0593feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree_split = sklearn.tree.DecisionTreeClassifier(**tree_params, ccp_alpha=ccp_alpha)\n",
    "    tree_split.fit(X_train, y_train)\n",
    "    trees.append(tree_split)\n",
    "\n",
    "acc_scores = [sklearn.metrics.accuracy_score(y_test, tree.predict(X_test)) for tree in trees]\n",
    "\n",
    "tree_depths = [tree.tree_.max_depth for tree in trees]\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(ccp_alphas[:-1], acc_scores[:-1])\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"Accuracy scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696c85d-98cb-4b4a-9034-63cf87479c5a",
   "metadata": {},
   "source": [
    "We can then use the best effective alpha value to train an optimally-pruned tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906ad8e-02eb-4b5c-ac29-44d848361f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_alpha_best = ccp_alphas[np.argmax(acc_scores)]\n",
    "\n",
    "tree_split = sklearn.tree.DecisionTreeClassifier(**tree_params, ccp_alpha=ccp_alpha_best)\n",
    "tree_split.fit(X_train, y_train)\n",
    "\n",
    "print(\"Depth: \", tree_split.get_depth())\n",
    "print(\"Leaf nodes: \", tree_split.get_n_leaves())\n",
    "print(f\"Accuracy: {tree_split.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583959a6-9a51-4a15-b847-fe97eb56cd2e",
   "metadata": {},
   "source": [
    "Note the improved accuracy value, and the reduction in leaf nodes.\n",
    "\n",
    "You can read more about this type of complexity pruning [here](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07355cd7-3903-4e5c-8169-c8b587193063",
   "metadata": {},
   "source": [
    "### Part 4 - Bagging and Random Forests\n",
    "\n",
    "As discussed in the lecture, we can collect many decision trees into an ensemble by bootstrapping the training dataset, then aggregating their predictions into a single result. This allows our ensemble model to generalise better than a single tree and prevents overfitting, since each tree only sees a subset of the training data. This approach is called _Bagging_.\n",
    "\n",
    "One further method used when training decision trees for an ensemble algorithm is to only allow a subset of the available features to be used for splitting at each node of each tree. This means that at each node, a tree only gets to choose from a random set of features to split with. The reasoning for this is if one particularfeature ends-up being a very strong predictor, then it will be chosen much more regularly across the whole ensemble. If this feature is not allowed to participate in some random subset of trees, then we will end up with a more diverse ensemble. When this method is used, our ensemble is called a Random Forest.\n",
    "\n",
    "The notation for this is that out of a total number of features (or predictors) $p$, we choose some integer value $m$ where $m <= p$. $m$ is a hyperparameter, i.e. one that we choose before training. Typically we choose $m = round(sqrt(p))$. Bagging is the special-case of a Random Forest where $m = p$. We can therefore tweak the value of $m$ (the `max_features` parameter in sklearn) to create either a Bagged ensemble, or a full Random Forest.\n",
    "\n",
    "Let’s try training a bagged ensemble first, where $m = p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a1577-b050-4129-ba7a-0bd7b17243fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_params = {\n",
    "    **tree_params,\n",
    "    \"oob_score\": True,\n",
    "    \"n_estimators\": 100, \n",
    "    \"max_features\": len(X_train.columns),\n",
    "}\n",
    "\n",
    "forest = sklearn.ensemble.RandomForestClassifier(**forest_params)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(f\"OOB Error: {(1-forest.oob_score_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ec7a5-d128-4096-a3e8-60b00505d518",
   "metadata": {},
   "source": [
    "From above, you should see:\n",
    "- We specified the number of trees `n_estimators` to 100, which is the default. Larger values will take longer to train (measure it to see). As we discussed in the lecture, its possible to parallelise this training in order to address the scaling when training larger ensembles. This can be set with `n_jobs`\n",
    "- The bag has an associated OOB error rate of 20.31%. Since each tree only receives a bootstrapped subset of the total training data (the \"in-bag\" data), we can use all the data points it was _not_ trained with to get a measure of its accuracy. The Out-of-Bag (OOB) error is calculated by doing this for every tree in the ensemble and averaging the result.\n",
    "\n",
    "Similar to what we did previously, we can calculate the misclassification error rate by asking the ensemble to predict values for the testing set. Since we have many trees which each contribute a vote, the `predict` function automatically calcluates a majority result across the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9f7bc-8133-40ac-a91e-50ffba3a3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(\n",
    "    forest.predict(X_test),\n",
    "    y_test,\n",
    "    margins=True,\n",
    "    rownames=[\"Predicted\"],\n",
    "    colnames=[\"Actual\"],\n",
    "))\n",
    "\n",
    "print(f\"Accuracy: {forest.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e962959-b6ff-4ffb-ab21-ea9afe71f5a8",
   "metadata": {},
   "source": [
    "Now lets set $m = sqrt(p)$ (the default) and see the effect on the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d65d19-992c-47a0-a2ff-d64a4abc263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_params[\"max_features\"] = \"sqrt\"\n",
    "forest = sklearn.ensemble.RandomForestClassifier(**forest_params)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(f\"OOB Error: {(1-forest.oob_score_)*100:.2f}%\")\n",
    "display(pd.crosstab(\n",
    "    forest.predict(X_test),\n",
    "    y_test,\n",
    "    margins=True,\n",
    "    rownames=[\"Predicted\"],\n",
    "    colnames=[\"Actual\"],\n",
    "))\n",
    "print(f\"Accuracy: {forest.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5fc09-0f8c-40a9-a103-64da25cc1040",
   "metadata": {},
   "source": [
    "You can see that the OOB error has dropped slightly, and the accuracy has improved to 81%.\n",
    "\n",
    "One final thing we can investigate is the relative feature importance of the Random Forest. With a single Decision tree, we can easily interpret it to see which variables have the largest impact on the output. This is not feasible with ensemble methods though. Instead, we can calculate the variable importances for the ensemble using [\"Mean decrease in impurity (MDI)\"](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importance-based-on-mean-decrease-in-impurity). This is a representation of the relative predictive power of each feature, based on what affect they have on reducing the impurity during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20192dab-7131-4857-8855-50abd3d029a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(forest.feature_importances_, index=X.columns)\n",
    "display(forest_importances.sort_values(ascending=False))\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b175b9f-a416-48c8-8767-e2fff35eab85",
   "metadata": {},
   "source": [
    "Compare the relative variable importance to the plot of our Decision Tree. Do the same variables appear\n",
    "near the top in both cases?\n",
    "\n",
    "**Next steps**\n",
    "\n",
    "If time permits:\n",
    "- Tweak the value of `n_estimators` in the forest and measure the effect on the final accuracy\n",
    "- Train more classifiers like above using another dataset, e.g., the breast cancer dataset (`sklearn.datasets.load_breast_cancer()`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653d7f6-7058-4ed5-939e-988a34bbdaee",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. This lab is based on a practical from An Introduction to Statistical Learning\n",
    "1. [Raileanu and Stoffel, 2004 Theoretical comparison between the Gini Index and Information Gain criteria](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
