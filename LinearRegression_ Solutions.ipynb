{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Python \n",
    "#### Data Analytics with High Performance Computing\n",
    "\n",
    "This brief practical is intended to provide you with a starting point to experiment with some aspects of linear regression. It's also worth trying to see whether you can convert some of the content from lectures that is expressed into mathematical notation into code.\n",
    "\n",
    "As you evaluate the cells below, try to make sure you understand what's happening at each stage. I encourage you to tweak the code to experiment with what happens if you do things slightly differently (e.g. if you swap Series with slices from DataFrames, etc.).\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "We'll start by looking at the familiar 2D case (where $y$ is a function of a single variable $x$).\n",
    "\n",
    "First, import the necessary packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas to give us dataframes\n",
    "import seaborn as sns #seaborn for some prettier plots\n",
    "import numpy as np #used later for matrix manipulation\n",
    "\n",
    "import matplotlib.pyplot as plt # We'll use matplotlib to create some plots of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from `cats.csv` into a DataFrame called `cats`. This dataet contains the body weight and heart weight of cats as seen in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats=pd.read_csv(\"cats.csv\")\n",
    "cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the body weight against the heart weight. What so we see? Is there a relationship between the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(cats[\"Bwt\"],cats[\"Hwt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can fit a linear regression model using **scikit learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store our result in `reg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=LinearRegression().fit(cats[['Bwt']],cats[['Hwt']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the coefficients of the mdoelusing the `reg` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.intercept_)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a brief aside, we can also see the arguments which `fit` was originally called with, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually get a predicted value of $y$ for each $x$ point using the normal formula for a straight line, $y=\\theta_0+ \\theta_1 x$ where $y$ corresponds to the *predicted* value for `Hwt`, $x$ is the `Bwt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=reg.intercept_ + cats[['Bwt']]*reg.coef_\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll know plot the original data, and superimpose the straight line that comes from `ypred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats[\"Bwt\"],cats[\"Hwt\"])\n",
    "plt.plot(cats[\"Bwt\"],ypred[\"Bwt\"], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of evaluating the formula itself, you can use the built in `predict` method, which gives the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ypred2=reg.predict(cats[['Bwt']])\n",
    "#ypred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats['Bwt'],cats['Hwt'])\n",
    "plt.plot(cats['Bwt'],ypred2, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've allowed LinearRegression.fit() to find our straight line that best fits the data. To convince yourself that what you're plotting is a straigh line with the calculated slope and intercept, try changing the values of a and b below, recalcuating ymanual and replotting the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0=-0.5\n",
    "theta1=4\n",
    "ymanual= theta0+theta1*cats[['Bwt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cats['Bwt'],cats['Hwt'])\n",
    "plt.plot(cats['Bwt'],ypred['Bwt'], color='red')\n",
    "plt.plot(cats['Bwt'],ymanual['Bwt'], color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on in this practical, we're going to see how you can find the values of $\\theta_0$ and $\\theta_1$ that give the best fit from first principles, instead of using `fit()` from `sklearn`.\n",
    "\n",
    "### Calculating the Cost Function\n",
    "\n",
    "You will remember that Linear Regression works by minimising a **cost function** based on the **Mean Squared Error**. \n",
    "\n",
    "The cost function is calculated repeatedly in the process of fitting (or, to use Machine Learning terminology, in learning the model).\n",
    "\n",
    "Let's calculate the Mean Square Error here. We're looking at a simple case here where the target variable, $y$, depends on only one variable, $x$.\n",
    "\n",
    "Our model can be expressed as $ h_\\theta(x) = \\theta_0+\\theta_1 x $ and the records in the dataset above can be expressed as the set $\\{(x^{(i)},y^{(i)})\\}$ where $i$ runs from $1$ to $m$. Also, our cost functio is given by:\n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Write some code which calculates this quantity for the dataset above. Remember $h_\\theta(x)$ is the function for the straight line. I'm not asking here for code to *minimise* this function, just to *calculate* it given your straight line (defined by your parameters $\\mathbf{\\theta}$) and the original data points.\n",
    "\n",
    "**Hint:** Depending on your approach, you may need to convert a pandas dataframe to a numpy array. If you need to do this you can use `to_numpy()`.\n",
    "\n",
    "You should get the answer `2.08009128`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1= reg.coef_\n",
    "theta0= reg.intercept_\n",
    "\n",
    "#answer\n",
    "h=theta0+theta1*cats[[\"Bwt\"]]\n",
    "cost_function=np.sum((h[\"Bwt\"]-cats[\"Hwt\"])**2)/144\n",
    "\n",
    "print(cost_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, you can also calculate the mean square error using sklearn. Note that I said that cost function above was *based* on the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(cats[\"Hwt\"],ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some quick exploratory data analysis\n",
    "\n",
    "We are now going to look at the public dataset *mtcars*. This data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). Import the mtcars dataset using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV mtcars\n",
    "mtcars=pd.io.parsers.read_csv(\"mtcars.csv\",index_col=0) # this function helps us preserve the rownames of the dataset as row names instead of assigning them to a new column\n",
    "mtcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can view basic descriptive statistics of this dataset using the `describe` and `info` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtcars.shape)\n",
    "\n",
    "print(\"=====\")\n",
    "\n",
    "mtcars.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often informative to visualise data like this with a scatterplot matrix to get a quick impression of the data before exploring in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn provides a version that is a bit prettier out-of-the-box. This is a good way to roughly determine if there is any linear correlation between any pair of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can subset the plot to those variables that seem to have a linear relationship/are correlated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(mtcars[['mpg','wt','disp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correlation between these 3 variables in order to better undesratnd any association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson's correlation coefficient for mpg and disp\n",
    "print(mtcars['mpg'].corr(mtcars['disp']))\n",
    "\n",
    "print(\"========\")\n",
    "\n",
    "#Correlation matrix for mpg, disp, wt\n",
    "print(mtcars[['mpg','disp', 'wt']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression with Scikit Learn\n",
    "\n",
    "Now we will explore linear regression using more than one variable.\n",
    "\n",
    "While simple linear regression gives a model corresponding a straight **line**, linear regression with two independent variables gives a model corresponding to a flat **plane** (and in general, in higher dimensions, a so-called hyperplane). The \"height\" of the plane corresponds to the value of the hypothesis function which defines the model.\n",
    "\n",
    "We're going to continue to work with the subset of the mtcars dataset that we looked at above, and in particular we will try to predict the number of miles a car can drive for each gallon of petrol (`mpg`) using the weight (`wt` in lb/1000) of the car and its engine displacement (`disp` in cu.in.). Let's visualise the resulting plane below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter( mtcars['wt'], mtcars['disp'],mtcars['mpg'], c='blue', s=60)\n",
    "\n",
    "# create a grid of values in the wt (p1) and disp (p2) directions\n",
    "# to help us draw a plane\n",
    "p1, p2 = np.mgrid[1:6, 0:500:100] \n",
    "\n",
    "theta0=30 \n",
    "theta1=-3.3\n",
    "theta2=-0.015\n",
    "\n",
    "\n",
    "plane= theta0 + theta1*p1 + theta2*p2\n",
    "\n",
    "ax.plot_surface(p1,p2,plane,color='red',alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the values of $\\theta_0, \\theta_1, \\theta_2$ and see how this changes the fitted plane in the plot above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets find the optimal parameters for this model (those that give the \"best fit\") without using library functions but using the analytical solution provided by the normal equuations, $\\boldsymbol{\\theta}=(X^T X)^{-1} X^T y$\n",
    "\n",
    "We will now try to use the matrix version of the normal equation (as described in lectures) to find the optimal parameters for this model. The matrix version requires two matrices X and y defined as:\n",
    "\n",
    "$$ X =\n",
    " \\begin{pmatrix}\n",
    "  1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "  1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "  \\vdots  & \\vdots & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\\\\n",
    "\\end{pmatrix}, y = \n",
    "\\begin{pmatrix}\n",
    "  y^{(1)} \\\\\n",
    "  y^{(2)} \\\\\n",
    "  \\vdots \\\\\n",
    "  y^{(m)}\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the matrices $\\boldsymbol{y}$, $\\boldsymbol{X}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here\n",
    "\n",
    "X=mtcars[['wt','disp']]\n",
    "X[\"col1\"]=1\n",
    "print(X.head())\n",
    "\n",
    "# bring itno right order\n",
    "X=X[[\"col1\",\"wt\",\"disp\"]]\n",
    "print(X.head())\n",
    "\n",
    "y=mtcars['mpg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to solve for the parameters using the equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}=(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "**TASK**: Build up this equation in Python. We _could_ convert things back to NumPy at the start (and for high performance with larger data, this might be desirable) but you can do some things like the transpose with Pandas direclty. The inverse, on the other hand, is easier to do in NumPy with `np.linalg.inv()`.\n",
    "\n",
    "Here are some hints:\n",
    "\n",
    "1. Both dataframes and numpy arrays have a transpose method e.g. `M.transpose()`\n",
    "2. Matrix multiplication can be performed with both dataframes and numpy arrays using the dot method, e.g. `A.dot(B)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indert your code here\n",
    "\n",
    "theta=np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Now see if you can use these coefficients to predict the mpg for the Volvo 142E using the weight and displacement figures shown above for that vehicle.\n",
    "\n",
    "Hint:\n",
    "\n",
    "- You can use matrix multiplication to multiply a vector containing the required values by the co-efficients matrix. Recall that,  $\\hat{y}=X \\hat{\\theta}$.\n",
    "\n",
    "The predicted value for the Volvo 142E should be 23.50057 miles per gallon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indert your code here\n",
    "\n",
    "X.loc['Volvo 142E'].dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression using StatsModels and built in functions\n",
    "\n",
    "We have been doing things mostly \"by hand\" above to make sure you understand a little bit about how linear regression is implemented.\n",
    "\n",
    "In practice, we usually use existing methods, as long as we know exctly what these do and calculate, and the same functions used above for single variable linear regression also work for multiple variable regression.\n",
    "\n",
    "The library StatsModels comes with a variety of measures already calculated and easily accessible as well. Let's import the library and start fiting our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scikit learn\n",
    "reg_1=LinearRegression().fit(mtcars[['wt','disp']],mtcars['mpg'])\n",
    "\n",
    "# using statsModels\n",
    "reg_2=smf.ols('mpg ~ wt + disp', data=mtcars).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the coefficent estimates as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_1.intercept_)\n",
    "print(reg_1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the documentation for the model object returned to see some of the other information that's returned in the object\n",
    "\n",
    "https://www.statsmodels.org/stable/regression.html\n",
    "\n",
    "https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automatically extract the fitted values using `reg2.fittedvalues`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_2.fittedvalues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reg_2.resid` contains the residuals, i.e., the differences between true and fitted values. Recall the noise $\\epsilon$ we talked about in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_2.resid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to calculate the sum of the squared errors of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer\n",
    "\n",
    "np.sum(reg_2.resid**2)\n",
    "#or reg_2.ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Adjusted R squared, `reg2.rsquared_adj`,  and do some plots to assess the model fit and whether certain assumptions of the model hold. Some of the things to look for:\n",
    "\n",
    "- Normality of the residuals (look at a hstpgram of the residuals)\n",
    "- Uncorrelated residuals   (look at an autocorrelation (ACF) plot of the residuals, e.g., pd.plotting.autocorrelation_plot())\n",
    "- Plot of fitted values vs actual values\n",
    "- Plot of fitted values vs Residuals, looking for constant variance of the residuals, i.e. is there any pattern in the residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "plt.hist(reg_2.resid)\n",
    "\n",
    "pd.plotting.autocorrelation_plot(reg_2.resid)\n",
    "\n",
    "plt.scatter(reg2.fittedvalues, mtcars['mpg'])\n",
    "\n",
    "plt.scatter(reg2.fittedvalues,reg2.resid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you know what an autocorrelation is and how to interpret an autocorrelation plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `summary` on the object `reg2` you can get more information about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information does this table give you?\n",
    "\n",
    "The top left column gives you information about the dependent variable, the method use dfor fitting, number of observations used, and the degrees of freedom of the model. \n",
    "\n",
    "The right column prints out out some key statistics such as $R^2$, adjusted $R^2$, log likelihood value and the $F$-statistic along with its associated $p$-value, Prob($F$-Statistic). The $F$-statistic and corresponding $p$-value are used to assess whethewr all the coefficients of the model are jointly equal to zero against the hypotheis that at least one is different than zero. If the reported $p$-value is greater than your predefined significance level then there is not sufficient evidence to support the model. The significanc elevel we usually use is $0.05$, i.e., $5\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below these we get all the information we need regarding the coefficients estimates, their standard error (std err) and at the end a $95\\%$ confidence interval for each coefficient estimate.  The column named $t$ holds a t-statistic for each coefficient estimate ($t$-value = coefficient estimate/ Standard error) in order to perform a statistical hypothesis test for each coefficent. These hypotheses tests, assess whether there is evidence that a coefficent can be zero given that all other explanatory variables/features are included in the model.\n",
    "\n",
    "For each $t$-value, a $p$-value is reported in the column P>|t|. If this value is higher than your defined significance level then there is not sufficient evidence that the corresponding coefficient is different than zero given that all other variables are included in the model. Similar conclusions we can draw by looking at the confidence interval of each coefficient estimate and see if it includes 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Polynomial regression can be used to fit non-linear models to data. It uses linear regression and simply adds polynomial terms to the set of feature values.\n",
    "\n",
    "Consider the following data set of tortoise shell length and clutch size (number of eggs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tortoise=pd.read_csv('tortoise.csv', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tortoise.plot(kind=\"scatter\", x='length', y='clutch', colormap=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data source: Ashton, K.G., R.L. Burke and J.N. Layne. 2007. Geographic variation in body and clutch size of gopher tortoises. Copeia 2007: 355-363._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like there is an obvious linear fit to this data but let's try to use linear regression to predict clutch size from length and plot the data with the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_1=smf.ols('clutch ~ length', data=tortoise).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "tortoise.plot(kind='scatter', x='length', y='clutch')\n",
    "plt.plot(tortoise.length,fit_1.fittedvalues, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this look like a great fit to the data? Calculate the sum of square errors, it should be $186.152$, print and evaluate its statistics. What can we conclude from the R-squared statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here\n",
    "\n",
    "# calculate sum of square errors\n",
    "print(np.sum(fit_1.resid**2))\n",
    "#or fit_1.ssr\n",
    "\n",
    "# summary statistics\n",
    "print(fit_1.summary())\n",
    "\n",
    "\n",
    "# Here are a number of different approaches to calculate what we refer to as the mean squared error:\n",
    "##print(mean_squared_error(tortoise.clutch, fit_1.fittedvalues))\n",
    "##print(mean_squared_error(tortoise.clutch, ypred))\n",
    "##J=0\n",
    "##for i in range(len(tortoise)):  \n",
    "##    J=J+(tortoise.clutch.iloc[i]-fit_1.fittedvalues.iloc[i])**2\n",
    "    \n",
    "##J=J/(len(tortoise))\n",
    "##print(J)\n",
    "##print(fit_1.ssr/len(tortoise))\n",
    "##print(fit_1.mse_resid/18*16)\n",
    "\n",
    "\n",
    "\n",
    "#reg=LinearRegression().fit(tortoise.iloc[:,0:1],tortoise.iloc[:,1:2])\n",
    "#ypred=reg.predict(tortoise.iloc[:,0:1])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better with polynomial regression without seriously overfitting the model by adding the square of the length as an additional feature to the feature set? Create an extra column in the dataframe that holds lengthSq, i.e. length squared, and fit a new model, fit_2, including this in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "tortoise['lengthSq']= tortoise['length']**2\n",
    "fit_2=smf.ols('clutch ~ length + lengthSq', data=tortoise).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fitted values onto the graph with lines between them to quickly show the shape of the new polynomial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "tortoise.plot(kind='scatter', x='length', y='clutch', colormap=None)\n",
    "plt.plot(tortoise.length,fit_1.fittedvalues, 'r')\n",
    "plt.plot(tortoise.length,fit_2.fittedvalues, 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seem's to capture the data better and reflects that very large tortoises tend to lay fewer eggs than middle sized tortoises. Use `summary()` to evaluate its statistics. What can we conclude from the R-squared statistic, compared to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
